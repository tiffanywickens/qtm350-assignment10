{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10: Parallel Computing\n",
    "\n",
    "### Due 19 November 2025\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This assignment is about parallel computing with Dask. You should use Python to implement the calculations. If possible, please submit your answers in PDF  or HTML format. In case you have any issues installing Dask via pip, please use the following command:\n",
    "\n",
    "```bash\n",
    "python -m pip install \"dask[complete]\" --use-deprecated=legacy-resolver\n",
    "```\n",
    "\n",
    "This command will resolve dependencies and install the required packages for Dask.\n",
    "\n",
    "You can also install Dask using conda (which the authors recommend):\n",
    "\n",
    "```bash\n",
    "conda install dask\n",
    "```\n",
    "\n",
    "If you encounter any issues, please check their website: <https://docs.dask.org/en/stable/install.html> and let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explain the concept of \"overhead\" in parallel computing with `joblib`. Why might running a very simple task (like adding 1 to a number) in parallel with `joblib` be slower than running it serially?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here: Overhead in parallel computing refers to the additional time and resources required to manage and coordinate multiple processes or threads. This includes the time taken to start up worker processes, distribute tasks, communicate between processes, and gather results. \n",
    "\n",
    "When running a very simple task, such as adding 1 to a number, the actual computation time is extremely short. However, the overhead associated with setting up parallel execution can be significant compared to the time taken for the computation itself. As a result, the total time taken for the parallel execution (including overhead) can exceed the time taken for serial execution, making it slower overall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a Python function `count_vowels(text)` that counts the vowels (a, e, i, o, u, case-insensitive) in a given string. Then, use the `Parallel` and `delayed` functions from the `joblib` library to apply your function in parallel. Use all available cores.  The function should return a list of integers, where each integer corresponds to the number of vowels in the respective sentence.\n",
    "\n",
    "```python\n",
    "sentences = [\n",
    "    \"Joblib makes parallel computing easy\",\n",
    "    \"Dask scales Python code effectively\",\n",
    "    \"Parallelism can speed up computations\",\n",
    "    \"Always consider the overhead\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 10, 13, 10]\n"
     ]
    }
   ],
   "source": [
    "## Your answer here\n",
    "def count_vowels(text):\n",
    "    vowels = \"aeiouAEIOU\"\n",
    "    count = 0\n",
    "    for ch in text:\n",
    "        # if ch is a vowel, increase count\n",
    "        if ch in vowels:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "sentences = [\n",
    "    \"Joblib makes parallel computing easy\",\n",
    "    \"Dask scales Python code effectively\",\n",
    "    \"Parallelism can speed up computations\",\n",
    "    \"Always consider the overhead\",\n",
    "]\n",
    "\n",
    "# use all cores: n_jobs=-1\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(count_vowels)(s) for s in sentences\n",
    ")\n",
    "\n",
    "print(results)  # list of vowel counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Write a function called `get_length` that takes a word as input and returns its length. Then, using the provided list `words`, do the following:\n",
    "\n",
    "* Use a standard (sequential) for loop to calculate the length of each word by calling your function.\n",
    "* Use the `joblib` library to calculate the length of each word in parallel, also calling your function. Use `Parallel` and `delayed` from `joblib` again.\n",
    "* Compare the syntax of the sequential and parallel approaches. How do they differ when writing the loop?\n",
    "\n",
    "```python\n",
    "words = [\"joblib\", \"parallel\", \"computing\", \"example\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential: [6, 8, 9, 7]\n",
      "Parallel: [6, 8, 9, 7]\n"
     ]
    }
   ],
   "source": [
    "## Your answer here\n",
    "def get_length(word):\n",
    "    return len(word)\n",
    "words = [\"joblib\", \"parallel\", \"computing\", \"example\"]\n",
    "\n",
    "lengths_seq = []\n",
    "for w in words:\n",
    "    lengths_seq.append(get_length(w))\n",
    "\n",
    "print(\"Sequential:\", lengths_seq)\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "lengths_par = Parallel(n_jobs=-1)(\n",
    "    delayed(get_length)(w) for w in words\n",
    ")\n",
    "\n",
    "print(\"Parallel:\", lengths_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create a 10000x10000 Dask array `da_a` filled with random integers between 0 and 100, chunked into (500, 1000) blocks. Use `RandomState(350)` to make your code reproducible. Create a second Dask array `da_b` of the same shape and chunks, filled with ones. Compute `da_c = (da_a + da_b) * 2` and its mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.992538"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Your answer here\n",
    "import dask.array as da\n",
    "\n",
    "# random integers 0â€“100, using a fixed RandomState\n",
    "rs = da.random.RandomState(350)\n",
    "da_a = rs.randint(\n",
    "    low=0,\n",
    "    high=100,\n",
    "    size=(1000, 1000),\n",
    "    chunks=(500, 1000),\n",
    ")\n",
    "\n",
    "# array of ones with same shape/chunks\n",
    "da_b = da.ones_like(da_a)\n",
    "\n",
    "# elementwise expression\n",
    "da_c = (da_a + da_b) * 2\n",
    "\n",
    "# mean is a Dask scalar, must be computed\n",
    "mean_value = da_c.mean().compute()\n",
    "mean_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What is the difference between `dask.dataframe.compute()` and `dask.dataframe.persist()`? When would you typically use `.persist()`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here: The difference between `dask.dataframe.compute()` and `dask.dataframe.persist()` lies in how they handle the computation of Dask DataFrames. `dask.dataframe.compute()` triggers the execution of the entire computation graph and returns the result as a concrete Pandas DataFrame. It is typically used when you want to obtain the final result of your computations and are ready to bring the data into memory. On the other hand, `dask.dataframe.persist()` triggers the computation but keeps the result as a Dask DataFrame, storing it in distributed memory. This allows for faster subsequent computations on the persisted data without recomputing from scratch. You would typically use `.persist()` when you have a large dataset that you want to keep in memory for multiple operations, as it can significantly speed up subsequent computations by avoiding redundant calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In this question, you will compare the performance of a regular `for` loop and `dask` for a simple computation. First, create a function called `intensive_task` as follows:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import time\n",
    "import dask\n",
    "\n",
    "def intensive_task(n):\n",
    "    loop_limit = 10_000_000 # How many iterations inside the function\n",
    "    total = 0\n",
    "    for i in range(loop_limit):\n",
    "        total += i*i\n",
    "    return total\n",
    "```\n",
    "\n",
    "Then, create a list called `inputs` with 6 values:\n",
    "\n",
    "```python\n",
    "inputs = [1, 2, 3, 4, 5, 6] \n",
    "```\n",
    "\n",
    "Now, use the function `time.time()` to measure the time it takes to run the function `intensive_task` for each value in the list `inputs` using a regular `for` loop. Store the results in a list called `results`. Remember to create the `start_time` and `end_time` variables to measure the time taken for the computation. The result, which is the difference between `end_time` and `start_time`, should be printed.\n",
    "\n",
    "Repeat the same task using `dask`. However, instead of using the `@dask.delayed` decorator, use the code below:\n",
    "\n",
    "```python\n",
    "tasks = [dask.delayed(intensive_task)(i) for i in inputs]\n",
    "```\n",
    "\n",
    "Then, use `dask.compute()` to compute the results. Again, measure the time taken for the computation and print the result. Which one is faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential time: 2.594106674194336\n",
      "Dask time: 2.3910107612609863\n"
     ]
    }
   ],
   "source": [
    "## Your answer here\n",
    "import numpy as np\n",
    "import time\n",
    "import dask\n",
    "\n",
    "def intensive_task(n):\n",
    "    loop_limit = 10_000_000\n",
    "    total = 0\n",
    "    for i in range(loop_limit):\n",
    "        total += i * i\n",
    "    return total\n",
    "\n",
    "inputs = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results_seq = []\n",
    "for x in inputs:\n",
    "    results_seq.append(intensive_task(x))\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Sequential time:\", end_time - start_time)\n",
    "tasks = [dask.delayed(intensive_task)(i) for i in inputs]\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results_dask = dask.compute(*tasks)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Dask time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. In the same folder as this notebook, you will find a Parquet file named `data.parquet`. It is available here: <https://github.com/danilofreire/qtm350/blob/main/assignments/data.parquet>. This file contains student records with the following columns:\n",
    "\n",
    "* `emory_id` (integer) \n",
    "* `student_name` (string)\n",
    "* `major` (string)\n",
    "* `gpa` (float)\n",
    "\n",
    "Write Python code using `dask.dataframe` to read the `data.parquet` file, but only load the `major` and `gpa` columns. Then, print the first 5 rows of the resulting Dask DataFrame using the `.head()` method, and calculate the average GPA by major.\n",
    "\n",
    "You will need a Parquet engine to read the file. If you don't have one installed, you can use `pyarrow`. You can install it using conda (or pip):\n",
    "\n",
    "```bash\n",
    "conda install pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       major   gpa\n",
      "0    History  2.98\n",
      "1  Chemistry  3.16\n",
      "2  Chemistry  3.83\n",
      "3        QTM  3.67\n",
      "4    CompSci  3.33\n",
      "major\n",
      "Biology      3.044000\n",
      "Chemistry    3.320000\n",
      "CompSci      3.352857\n",
      "Economics    3.285000\n",
      "English      3.484000\n",
      "History      3.098750\n",
      "Physics      3.222857\n",
      "QTM          2.957500\n",
      "Name: gpa, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Your answer here\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# read only specific columns\n",
    "ddf = dd.read_parquet(\"data.parquet\", columns=[\"major\", \"gpa\"])\n",
    "\n",
    "# print first 5 rows\n",
    "print(ddf.head())\n",
    "\n",
    "# groupby major and compute mean gpa\n",
    "avg_gpa_by_major = ddf.groupby(\"major\")[\"gpa\"].mean().compute()\n",
    "print(avg_gpa_by_major)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. You have two CSV files in this directory:\n",
    "\n",
    "* `students.csv`: Contains columns `student_id`, `student_name`. Available here: <https://github.com/danilofreire/qtm350/blob/main/assignments/students.csv>.\n",
    "* `grades.csv`: Contains columns `student_id`, `course`, `grade`. Available here: <https://github.com/danilofreire/qtm350/blob/main/assignments/grades.csv>.\n",
    "\n",
    "Write Python code using dask.dataframe to:\n",
    "\n",
    "* Read `students.csv` into a Dask DataFrame called `ddf_students`.\n",
    "* Read `grades.csv` into a Dask DataFrame called `ddf_grades`.\n",
    "* Merge these two DataFrames together based on the common `student_id` column. An inner merge is recommended (only include students present in both files).\n",
    "* From the merged DataFrame, select only the `student_name`, `course`, and `grade` columns. Save it as `ddf_final`.\n",
    "* Compute and print the first 5 rows of this final merged DataFrame using `.head()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good luck! ðŸ˜ƒ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
